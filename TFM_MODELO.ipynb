{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba26d539-ae9b-4872-8fba-3136d9ed7ebe",
   "metadata": {},
   "source": [
    "### 1. Modelo pre entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ffa4c044-9f4c-4803-b52a-64c756ed9772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='data_small_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0a5b1ba-e535-40b9-9a80-13c0e224d08d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'TARGET', 'FECHA_STRING', 'COMENTARIO', 'FECHA', 'HORA', 'COMENTARIO_LIMPIO', 'PALABRAS', 'TOKENS', 'NUM_TOKENS', 'sentimiento', 'comentario_tarea'],\n",
       "        num_rows: 30000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3cb30a8a-7c2b-4421-b735-26c71d3ded19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def train_test_split(dataset_dict: DatasetDict, test_size: float = 0.2) -> DatasetDict:\n",
    "    \"\"\"Split a dataset dictionary into train and test based on test size.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (DatasetDict): Input dataset dictionary.\n",
    "        test_size (float, optional): Fraction of data to include in the test set. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: Result dataset dictionary with the desired splitting strategy.\n",
    "    \"\"\"\n",
    "    train_test = dataset_dict[\"train\"].train_test_split(test_size=test_size)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_test[\"train\"],\n",
    "        \"test\": train_test[\"test\"]\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0113b482-58fe-4bed-97cc-ffd450678ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función para dividir un DatasetDict en train y test\n",
    "divided_data = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "648f8bcd-46ab-40b1-ab6d-95afc7b65b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_dataset = divided_data[\"train\"].shuffle(seed=42).select([i for i in list(range(10000))])\n",
    "dataset_test_dataset = divided_data[\"test\"].shuffle(seed=42).select([i for i in list(range(2000))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7e79b9b4-3e3b-4690-831d-f70180419aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({ \n",
    "    \"text\": dataset_train_dataset[\"COMENTARIO_LIMPIO\"], # Campo de texto \n",
    "    \"label\": dataset_train_dataset[\"TARGET\"] # Campo de sentimiento\n",
    "})\n",
    "\n",
    "test_ds = Dataset.from_dict({ \n",
    "    \"text\": dataset_test_dataset[\"COMENTARIO_LIMPIO\"], # Campo de texto \n",
    "    \"label\": dataset_test_dataset[\"TARGET\"] # Campo de sentimiento\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "858d5f22-6dca-46db-bf91-48a15201265c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoTokenizer\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cd79e796-1253-4a3c-a9f8-332f154cf9c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a4845767ee4718b02fcc087493b2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "671f816fd1a94b4db5f26f99bbb454e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64) #padding=True\n",
    " \n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62bf00d9-12f6-4638-ae7c-fd52ae93f908",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_with_labels = []\n",
    "\n",
    "for i in range(len(tokenized_train)):\n",
    "    text = tokenized_train[i][\"text\"]\n",
    "    label = tokenized_train[i][\"label\"]\n",
    "    texts_with_labels.append({\"text\": text, \"sentiment\": label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5bec463f-1d78-4aa8-8d05-c4a7b222775b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead,AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/flan-t5-small\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01fe814a-66fe-41b2-8e3f-3e58e77648fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0024, -0.0455]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4509,  0.1684]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2468,  0.1485]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2551,  0.0971]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2662,  0.1953]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4207,  0.1219]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2556,  0.1601]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1357,  0.3012]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3349,  0.1916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2810,  0.0896]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4469,  0.1593]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1929,  0.3326]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3553,  0.3172]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3512,  0.3059]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3973,  0.1914]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3516,  0.1133]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3597,  0.1674]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2341,  0.2190]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5379,  0.2563]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2830,  0.2032]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3110,  0.4163]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3173,  0.2000]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4402,  0.2175]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3417,  0.1449]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2543,  0.3428]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3521,  0.2741]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3735,  0.3142]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2412,  0.2579]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.0985,  0.2691]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4516,  0.1795]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3353,  0.3165]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3738,  0.1273]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3447,  0.2585]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2697,  0.3395]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2715,  0.1804]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2079,  0.4377]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3838,  0.0614]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4051,  0.4835]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3727,  0.3330]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2613,  0.2166]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1706, -0.0303]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3264,  0.1869]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3353,  0.2655]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4136,  0.1233]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3650,  0.2421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3171,  0.1728]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3318,  0.2197]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2339,  0.4030]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4597,  0.2859]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4842,  0.2962]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4798,  0.2246]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1699,  0.1994]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2073,  0.2484]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3279,  0.2738]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4002,  0.2263]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3711,  0.3441]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.5958,  0.1532]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3411,  0.1498]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4819,  0.2272]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3545,  0.2500]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3344,  0.2628]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2211,  0.3967]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3276,  0.2575]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3319,  0.2026]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2655,  0.2480]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4038,  0.4007]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2908,  0.4030]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4500,  0.2268]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3825,  0.1426]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3247,  0.4808]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3277,  0.2319]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4714,  0.3341]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3079,  0.0953]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4135,  0.2118]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3400,  0.2101]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2626,  0.1484]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2864,  0.2960]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3798,  0.1499]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4583,  0.1193]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3728,  0.3065]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4144,  0.2641]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2922,  0.2929]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3469,  0.2814]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2937,  0.0668]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2181,  0.1985]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1900,  0.1768]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4509,  0.3417]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1986,  0.4021]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3360,  0.2956]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2682,  0.1949]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.1860,  0.2240]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3204,  0.1765]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3917,  0.2685]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3567,  0.1948]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2447,  0.2088]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.2940,  0.3817]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3159,  0.4574]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.4433,  0.1578]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3549,  0.2347]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[-0.3460,  0.2235]], grad_fn=<AddmmBackward0>)\n",
      "F1-score con modelo preentrenado: 0.7096774193548387\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "f1_metric = load_metric(\"f1\")\n",
    "\n",
    "#def get_sentiment_list(texts_with_labels):\n",
    "predictions = []  # Lista para almacenar las etiquetas de sentimiento generadas por el modelo\n",
    "labels = []  # Lista para almacenar las etiquetas de sentimiento reales\n",
    "\n",
    "for item in texts_with_labels:\n",
    "    text = item[\"text\"]\n",
    "    true_sentiment = item[\"sentiment\"]\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    print(logits)\n",
    "    logits = outputs.logits\n",
    "    predicted_class = torch.argmax(logits, dim=1).item()\n",
    "    \n",
    "    predictions.append(predicted_class)\n",
    "    labels.append(true_sentiment)\n",
    "\n",
    "# Calcular el F1-score\n",
    "f1_score = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "\n",
    "# Mostrar el resultado\n",
    "print(f\"F1-score con modelo preentrenado: {f1_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e0fd605c-67b5-4873-b569-b7fdce80fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aquí termina la primera parte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d76111-5258-4325-91a6-33dd64d78af7",
   "metadata": {},
   "source": [
    "### 2. Fine tuning al modelo pre entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "22b2957f-336e-445e-a9b1-dcd55ab4c1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='data_small_train.csv')\n",
    "\n",
    "#dataset = load_dataset(train_dc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d869d1c8-57c0-4bb2-99db-e3fde0faaeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "def train_test_split(dataset_dict: DatasetDict, test_size: float = 0.2) -> DatasetDict:\n",
    "    \"\"\"Split a dataset dictionary into train and test based on test size.\n",
    "\n",
    "    Args:\n",
    "        dataset_dict (DatasetDict): Input dataset dictionary.\n",
    "        test_size (float, optional): Fraction of data to include in the test set. Defaults to 0.2.\n",
    "\n",
    "    Returns:\n",
    "        DatasetDict: Result dataset dictionary with the desired splitting strategy.\n",
    "    \"\"\"\n",
    "    train_test = dataset_dict[\"train\"].train_test_split(test_size=test_size)\n",
    "\n",
    "    return DatasetDict({\n",
    "        \"train\": train_test[\"train\"],\n",
    "        \"test\": train_test[\"test\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f68d377e-1be2-47c0-932d-773de6e82c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de la función para dividir un DatasetDict en train y test\n",
    "divided_data = train_test_split(dataset, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "88d24317-5ad4-4b82-95ac-1e39850318e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train_dataset = divided_data[\"train\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
    "dataset_test_dataset = divided_data[\"test\"].shuffle(seed=42).select([i for i in list(range(20))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ee1ab1cb-f090-4ff8-ad99-015c24884658",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_dict({ \n",
    "    \"text\": dataset_train_dataset[\"COMENTARIO_LIMPIO\"], # Campo de texto \n",
    "    \"label\": dataset_train_dataset[\"TARGET\"] # Campo de sentimiento\n",
    "})\n",
    "\n",
    "test_ds = Dataset.from_dict({ \n",
    "    \"text\": dataset_test_dataset[\"COMENTARIO_LIMPIO\"], # Campo de texto \n",
    "    \"label\": dataset_test_dataset[\"TARGET\"] # Campo de sentimiento\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bcd66e1d-72ce-4a5e-93e0-936a35f96a2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, AutoTokenizer\n",
    "\n",
    "#tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-small')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5317445a-0c30-47c0-b08d-98e9009fb0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b86804032894dd7a4d49989da1378da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96da86ac01744cc9ae8a0bcb1929040f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=64) #padding=True\n",
    " \n",
    "tokenized_train = train_ds.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "fee00f8f-e28f-46a9-9d20-cda3c2071922",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(ids) for ids in tokenized_train['input_ids'])\n",
    "for i in range(len(tokenized_train['input_ids'])):\n",
    "    while len(tokenized_train['input_ids'][i]) < max_length:\n",
    "        tokenized_train['input_ids'][i].append(0)\n",
    "        tokenized_train['attention_mask'][i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "e751e7e8-ca6b-4526-a1e5-72290e4d3dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max(len(ids) for ids in tokenized_test['input_ids'])\n",
    "for i in range(len(tokenized_test['input_ids'])):\n",
    "    while len(tokenized_test['input_ids'][i]) < max_length:\n",
    "        tokenized_test['input_ids'][i].append(0)\n",
    "        tokenized_test['attention_mask'][i].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8d3bc2b6-c17b-4221-b171-5dd0651c08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d25ea258-b252-41de-9cc8-6720d754488b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['classification_head.out_proj.weight', 'classification_head.out_proj.bias', 'classification_head.dense.weight', 'classification_head.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google/flan-t5-small\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "db60121c-65f8-4ee4-b8f5-79bc5708836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    load_f1 = load_metric(\"f1\")\n",
    "\n",
    "    logits, labels = eval_pred\n",
    "    \n",
    "    print(logits[0])\n",
    "    print(labels)\n",
    "    predictions = np.argmax(logits[0], axis=-1)\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1a31b3d3-ec7d-4aad-b109-eb50f2c7cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    " \n",
    "repo_name = \"finetuning-sentiment-model-3000-samples\"\n",
    " \n",
    "training_args = TrainingArguments(\n",
    "   output_dir=repo_name,\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=2,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    ")\n",
    " \n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,#tokenized_train\n",
    "   eval_dataset=tokenized_test,#tokenized_test\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c97774a2-db6a-4147-86e5-74b547cc375c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 00:37, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=0.7067760058811733, metrics={'train_runtime': 40.414, 'train_samples_per_second': 4.949, 'train_steps_per_second': 0.346, 'total_flos': 3404189030400.0, 'train_loss': 0.7067760058811733, 'epoch': 2.0})"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1af92d20-0ce4-4fc9-9a1a-1374eab7dc05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.10638631  0.22536479]\n",
      " [-0.22859465  0.24805708]\n",
      " [-0.27184883  0.17590147]\n",
      " [-0.35358506  0.17367108]\n",
      " [-0.28597197  0.10445598]\n",
      " [-0.3101298   0.27659613]\n",
      " [-0.32473022  0.26890147]\n",
      " [-0.32954744  0.16683611]\n",
      " [-0.35949075  0.04483829]\n",
      " [-0.2231046   0.15217641]\n",
      " [-0.30502173  0.43814382]\n",
      " [-0.3275834   0.28472427]\n",
      " [-0.2443533   0.41378346]\n",
      " [-0.2175438   0.24988937]\n",
      " [-0.40651688  0.13518102]\n",
      " [-0.18662673  0.3108778 ]\n",
      " [-0.3777661   0.07121117]\n",
      " [-0.2262328   0.23238806]\n",
      " [-0.16351596  0.26119712]\n",
      " [-0.21896645  0.19074056]]\n",
      "[1 0 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6771400570869446,\n",
       " 'eval_f1': 0.7499999999999999,\n",
       " 'eval_runtime': 2.1976,\n",
       " 'eval_samples_per_second': 9.101,\n",
       " 'eval_steps_per_second': 0.91,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518c68f5-52ff-4059-ba94-17974c2f70c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efc1a39-4583-49e5-a8aa-830e761b6abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "antoniokernel",
   "language": "python",
   "name": "antoniokernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
